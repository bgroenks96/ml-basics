{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Feed-Forward Neural Network\n",
    "\n",
    "We will start from the bottom up by defining a single neuron. A neuron should be able to do the following:\n",
    "\n",
    "1. Calculate the value of its net input from its input weights and the given input values.\n",
    "2. Calculate the value of its output from its activation function and the given net input.\n",
    "3. Update its weights according to some delta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias, activationFunc):\n",
    "        # A neuron has both a bias and a sequence of weights.\n",
    "        # We will leave the weights uninitialized for now.\n",
    "        self.bias = bias\n",
    "        self.activationFunc = activationFunc\n",
    "        self.weights = []\n",
    "    \n",
    "    # Here we calculate the total input value 'net' for this neuron.\n",
    "    # 'net' is simply the dot product of the given input vector with this neuron's weight vector.\n",
    "    def calculate_net_input(self, inputs):\n",
    "        assert inputs.shape == self.weights.shape\n",
    "        return np.dot(self.weights, inputs) + self.bias\n",
    "    \n",
    "    # Here we call our neuron's activation function with the given 'net' input value.\n",
    "    def activate(self, net_input):\n",
    "        return self.activationFunc(net_input)\n",
    "    \n",
    "    # Convenience function that will both calculate net inputs and return the result of activation.\n",
    "    def calculate_output(self, inputs):\n",
    "        net_input = self.calculate_net_input(inputs)\n",
    "        return self.activate(net_input)\n",
    "    \n",
    "    # And finally we allow for the weights to be updated according to some vector of delta values.\n",
    "    # 'learning_rate' is an arbitrary scalar that will be multiplied against the deltas.\n",
    "    def update(self, w_grads, b_grad, learning_rate):\n",
    "        self.weights -= learning_rate * w_grads\n",
    "        self.bias -= learning_rate * b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll move on to defining a single *neuron layer*. A layer is an arbitrary group of non-connected neurons that are each connected only to neurons in other layers. It can be thought of as a *partition* in terms of graph theory where each neuron is a component disjoint from the others in the layer. Our layer should be able to do the following:\n",
    "\n",
    "1. Initialize each of its neurons according to the shape of its input.\n",
    "2. \"Connect\" its output to the next layer in front of it.\n",
    "3. *Feed forward* its outputs to that layer.\n",
    "4. Collect the error for each of its neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, neuron_count, bias, activationFunc):\n",
    "        # Use bias and activationFunc to create the neurons for our\n",
    "        # layer. Everything else can stay unintialized for now.\n",
    "        self.neuron_count = neuron_count\n",
    "        self.outputs = np.zeros(neuron_count)\n",
    "        self.input_dims = -1\n",
    "        self.w_grads = []\n",
    "        self.b_grads = []\n",
    "        # 'receiverFunc' will be the \n",
    "        self.receiverFunc = None\n",
    "        self.neurons = [Neuron(bias, activationFunc) for i in xrange(neuron_count)]\n",
    "            \n",
    "    # Sets the shape of our layer's expected input by initializing all of\n",
    "    # the neurons' weight vectors to size 'input_dims'. Weight values are\n",
    "    # initially set to uniform random values between -1 and 1.\n",
    "    def initialize(self, input_dims, weight_values=None):\n",
    "        assert weight_values == None or len(weight_values) == input_dims\n",
    "        self.input_dims = input_dims\n",
    "        for (i, n) in enumerate(self.neurons):\n",
    "            n.weights = weight_values.copy() if weight_values != None else np.random.uniform(-0.5, 0.5, input_dims)\n",
    "    \n",
    "    # Connects 'receiver' to this layer to receive the results of 'feed_forward'.\n",
    "    # 'receiver' may be either a NeuronLayer, in which case it will be initialized\n",
    "    # to mach this layer's output shape, or it can be any function that expects a single\n",
    "    # argument 'inputs' where 'inputs' is an array of length 'neuron_count' for this layer.\n",
    "    # Note: if receiver is not a NeuronLayer, 'receiver_weight_values' is ignored.\n",
    "    def connect(self, receiver, receiver_weight_values=None):\n",
    "        assert receiver != None\n",
    "        if isinstance(receiver, NeuronLayer):\n",
    "            self.receiverFunc = lambda x: receiver.feed_forward(x)\n",
    "            receiver.initialize(self.neuron_count, receiver_weight_values)\n",
    "        else:\n",
    "            self.receiverFunc = receiver\n",
    "            \n",
    "    # Computes the output value for each neuron given 'inputs' and then\n",
    "    # \"feeds forward\" the results by calling 'receiverFunc' (if this layer\n",
    "    # is connected to a receiver).\n",
    "    def feed_forward(self, inputs):\n",
    "        assert len(inputs) == self.input_dims\n",
    "        for (i, n) in enumerate(self.neurons):\n",
    "            self.outputs[i] = n.calculate_output(inputs)\n",
    "        if self.receiverFunc != None:\n",
    "            self.receiverFunc(self.outputs)\n",
    "    \n",
    "    # Calculates the mean squared error for each neuron in this layer given the vector of expected output values.\n",
    "    def calculate_error(self, expected_outputs):\n",
    "        assert len(expected_outputs) == len(self.outputs)\n",
    "        return [0.5 * (expected_outputs[i] - self.outputs[i])**2 for (i, e) in enumerate(self.neurons)]\n",
    "    \n",
    "    # Updates the weights and biases for all of this layer's neurons from the given.\n",
    "    # w_grads should be a matrix of size NxM, where N is output dimensions of this layer and M is the input dimensions.\n",
    "    # b_grads should be a vector of size N, where N is the output dimensions of this layer.\n",
    "    def update(self, w_grads, b_grads, learning_rate):\n",
    "        assert w_grads.shape == (self.neuron_count, self.input_dims) and len(b_grads) == self.neuron_count\n",
    "        self.w_grads = w_grads\n",
    "        self.b_grads = b_grads\n",
    "        for (i, n) in enumerate(self.neurons):\n",
    "            n.update(w_grads[i], b_grads[i], learning_rate)\n",
    "            \n",
    "    def get_weights(self):\n",
    "        return [(n.weights, n.bias) for n in self.neurons]\n",
    "    \n",
    "    def dump_state(self):\n",
    "        print 'weights, bias: {0}'.format(self.get_weights())\n",
    "        print 'gradients: {0} + {1}'.format(self.w_grads, self.b_grads)\n",
    "        print 'outputs: {0}'.format(self.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our basic components, it's time for the fun part; the *nerual network*. Here we will combine multiple layers to form a fully connected, feed-forward network that we can train to fit some data.\n",
    "\n",
    "In principle, we could generalize this code to allow for an arbitrary number of hidden layers. We'll keep things simple, though, and just implement a basic 3-layer FFNN.\n",
    "\n",
    "*Note: \"3-layer\" includes the \"input layer\" which need not be represented by an actual NeuronLayer in the implementation. Traditionally, the \"input layer\" is just a way of visualizing how the inputs are fed into the hidden layer.*\n",
    "\n",
    "Our network should do the following:\n",
    "\n",
    "1. Initialize and connect each neuron layer.\n",
    "2. Implement backpropagation to allow for training.\n",
    "3. Provide a function to \"train\" the network on any number of training examples.\n",
    "4. Provide a function to \"predict\" one or more output values given some input (feed-forward only, no back prop).\n",
    "5. Learn a model that will cure cancer.\n",
    "\n",
    "Ok, maybe that last one is a bit ambitious. But we can be optimistic! :D\n",
    "\n",
    "Item number (2) is where things get interesting... we need to backpropagate the error from our feed-forward output through the network. Each weight should be updated with respect to its contribution to the total error. We compute this using partial derivatives.\n",
    "\n",
    "*Note: all multiplication $\\times$ operations here are element-wise, unless otherwise specified.*\n",
    "\n",
    "So for each weight $i$, we'll have:\n",
    "\n",
    "$w'_i = w_i - \\alpha \\times \\frac{\\partial E_{total}}{\\partial w_i}$\n",
    "\n",
    "where $w'_i$ is our new weight value and $E_{total} = \\sum_{i=0}^{N}\\frac{1}{2} \\times (o'_i - o_i)^2$\n",
    "where $o'_i$ is our expected output value and $o_i$ is our actual output value for some output $i$.\n",
    "\n",
    "For each layer, the partial derivative of the errors with respect to a single weight can be computed as:\n",
    "\n",
    "$\\frac{\\partial E_{total}}{\\partial w_i} = \\frac{\\partial E_{total}}{\\partial o_i}\\times \\frac{\\partial o_i}{\\partial net_i}\\times\\frac{\\partial net_i}{\\partial w_i}$\n",
    "\n",
    "For the output layer:\n",
    "\n",
    "$\\frac{\\partial E_{total}}{\\partial o_i} = -(o'_i - o_i)$\n",
    "\n",
    "$\\frac{\\partial o_i}{\\partial net_i} = o_i\\times (1 - o_i)$\n",
    "\n",
    "$\\frac{\\partial net_i}{\\partial w_i} = in_i$\n",
    "\n",
    "where $in_i$ is the input from the hidden layer at $i$\n",
    "\n",
    "For the hidden layer:\n",
    "\n",
    "$\\frac{\\partial E_{total}}{\\partial h_i} = \\sum_{j}\\frac{\\partial E_{o_j}}{\\partial h_i}$\n",
    "\n",
    "where $h_i$ is the output for the hidden layer at node $i$ and $E_{o_j}$ is the error for the output layer at node $j$.\n",
    "\n",
    "$\\frac{\\partial E_{o_j}}{\\partial h_i} = \\frac{\\partial E_{o_j}}{\\partial o_j}\\times \\frac{\\partial o_j}{\\partial net_{oj}}\\times\\frac{\\partial net_{oj}}{\\partial h_i}$\n",
    "\n",
    "We already have the \"delta terms\" $\\frac{\\partial E_{o_j}}{\\partial o_j}\\times \\frac{\\partial o_j}{\\partial net_{oj}}$ from computing the error for the ouptut layer, so all we need is $\\frac{\\partial net_{oj}}{\\partial h_i}$ for each hidden neuron $i$. Notice that we have two dimensions here, $i$ (hidden layer index) and $j$ (output layer index). We can collect all of the weights in a $N\\times M$ matrix $W$ (weights from hidden layer to output layer), where $N$ is the number of hidden dimensions and $M$ is the number of output dimensions.\n",
    "\n",
    "Then our vector of partial derivatives for error with respect to each hidden layer output can be computed as:\n",
    "\n",
    "$\\frac{\\partial E_{total}}{\\partial h_i} = \\sum_{j} W_{ij}\\times D$\n",
    "\n",
    "where $D$ is our vector of delta values from the output layer computation.\n",
    "\n",
    "$\\frac{\\partial net_{oj}}{\\partial h_i} = W_{ij}$\n",
    "\n",
    "for each hidden layer neuron and output layer neuron pair.\n",
    "\n",
    "You will find the implementation of this in the `_backpropagate_error` function in `SimpleNN` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now our neural net.\n",
    "class SimpleNN:\n",
    "    \n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, learn_rate=0.05, **args):\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.learn_rate = learn_rate\n",
    "        \n",
    "        # Process optional arguments\n",
    "        hidden_layer_weights = args['hidden_layer_weights'] if 'hidden_layer_weights' in args else None\n",
    "        output_layer_weights = args['output_layer_weights'] if 'output_layer_weights' in args else None\n",
    "        hidden_layer_bias = args['hidden_layer_bias'] if 'hidden_layer_bias' in args else 0.0\n",
    "        output_layer_bias = args['output_layer_bias'] if 'output_layer_bias' in args else 0.0\n",
    "        \n",
    "        # Define activation function for this FFNN implementation.\n",
    "        # Logistic \"sigmoid\" activation is a standard choice for basic neural nets.\n",
    "        logistic_activation = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        # Initialize our layers\n",
    "        self.hidden_layer = NeuronLayer(hidden_dims, hidden_layer_bias, logistic_activation)\n",
    "        self.output_layer = NeuronLayer(output_dims, output_layer_bias, logistic_activation)\n",
    "        self.hidden_layer.initialize(input_dims, hidden_layer_weights)\n",
    "        self.hidden_layer.connect(self.output_layer, output_layer_weights)\n",
    "        \n",
    "    # Evaluates the model at its current state by feeding the given inputs into the network\n",
    "    # and calculating the error in comparison to 'expected_outputs'. If 'return_mse' is True,\n",
    "    # the mean squred error (MSE) for each output value is returned. Otherwise, the error per-output\n",
    "    # is returned as an array.\n",
    "    def evaluate(self, inputs, expected_outputs, return_mse=True):\n",
    "        self.hidden_layer.feed_forward(inputs)\n",
    "        errors = self.output_layer.calculate_error(expected_outputs)\n",
    "        return errors if not return_mse else np.average(errors)\n",
    "            \n",
    "    def predict(self, inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.outputs\n",
    "    \n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        training_inputs = np.array(training_inputs)\n",
    "        training_outputs = np.array(training_outputs)\n",
    "        # If only one sample was given (in 1D form), reshape into 2D array with 1 row.\n",
    "        if len(training_inputs.shape) == 1:\n",
    "            training_inputs = np.reshape(training_inputs, (1, training_inputs.shape[0]))\n",
    "            training_outputs = np.reshape(training_outputs, (1, training_outputs.shape[0]))\n",
    "        \n",
    "        history = []\n",
    "        for (inputs, outputs) in zip(training_inputs, training_outputs):\n",
    "            mse = self.evaluate(inputs, outputs)\n",
    "            history.append((mse, self.output_layer.outputs.copy()))\n",
    "            \n",
    "            # Backpropagate error through network; we'll define this function below.\n",
    "            self._backpropagate_error(inputs, outputs)\n",
    "            \n",
    "        return history\n",
    "    \n",
    "    def dump_state(self):\n",
    "        print '{0} hidden dimensions, {1} output dimensions, {2} learning rate'.format(self.hidden_layer.neuron_count, self.output_layer.neuron_count, self.learn_rate)\n",
    "        print '=== Hidden Layer ==='\n",
    "        print self.hidden_layer.dump_state()\n",
    "        print '=== Output Layer ==='\n",
    "        print self.output_layer.dump_state()\n",
    "    \n",
    "    # Propagate output error with respect to expected_outputs back through the network using\n",
    "    # the backpropagation algorithm.\n",
    "    def _backpropagate_error(self, inputs, expected_outputs):\n",
    "        output_layer_deltas = self._calculate_output_layer_deltas(expected_outputs)\n",
    "        hidden_layer_deltas = self._calculate_hidden_layer_deltas(output_layer_deltas)\n",
    "        output_layer_grads = self._calculate_gradients(output_layer_deltas, self.hidden_layer.outputs, self.hidden_dims, self.output_dims)\n",
    "        self.output_layer.update(output_layer_grads, output_layer_deltas, self.learn_rate)\n",
    "        hidden_layer_grads = self._calculate_gradients(hidden_layer_deltas, inputs, self.input_dims, self.hidden_dims)\n",
    "        self.hidden_layer.update(hidden_layer_grads, hidden_layer_deltas, self.learn_rate)\n",
    "        \n",
    "#         for (i, o) in enumerate(self.output_layer.neurons):\n",
    "#             for (j, w) in enumerate(o.weights):\n",
    "#                 pd_error_wrt_weight = output_layer_deltas[i]*self.hidden_layer.outputs[j]\n",
    "#                 o.weights[j] -= self.learn_rate * pd_error_wrt_weight\n",
    "#             o.bias -= self.learn_rate * output_layer_deltas[i]\n",
    "#         for (i, h) in enumerate(self.hidden_layer.neurons):\n",
    "#             for (j, w) in enumerate(h.weights):\n",
    "#                 pd_error_wrt_weight = hidden_layer_deltas[i]*inputs[j]\n",
    "#                 h.weights[j] -= self.learn_rate * pd_error_wrt_weight\n",
    "#             h.bias -= self.learn_rate * hidden_layer_deltas[i]\n",
    "                \n",
    "    def _calculate_output_layer_deltas(self, expected_outputs):\n",
    "        deltas = np.zeros(self.output_dims)\n",
    "        for (i, n) in enumerate(self.output_layer.neurons):\n",
    "            pd_error_wrt_output = -(expected_outputs[i] - self.output_layer.outputs[i])\n",
    "            pd_output_wrt_net_input = self.output_layer.outputs[i] * (1 - self.output_layer.outputs[i])\n",
    "            deltas[i] = pd_error_wrt_output * pd_output_wrt_net_input\n",
    "        return deltas\n",
    "    \n",
    "    def _calculate_hidden_layer_deltas(self, output_layer_deltas):\n",
    "        deltas = np.zeros(self.hidden_dims)\n",
    "        for (i, h) in enumerate(self.hidden_layer.neurons):\n",
    "            pd_err_wrt_output = 0\n",
    "            for (j, o) in enumerate(self.output_layer.neurons):\n",
    "                pd_net_input_wrt_hidden_output = o.weights[i]\n",
    "                pd_err_wrt_output += output_layer_deltas[j] * pd_net_input_wrt_hidden_output\n",
    "            pd_hidden_output_wrt_net_input = self.hidden_layer.outputs[i] * (1 - self.hidden_layer.outputs[i])\n",
    "            deltas[i] = pd_err_wrt_output * pd_hidden_output_wrt_net_input\n",
    "        return deltas\n",
    "    \n",
    "    def _calculate_gradients(self, deltas, inputs, input_dims, output_dims):\n",
    "        w_grads = np.zeros((output_dims, input_dims))\n",
    "        for i in xrange(output_dims):\n",
    "            for j in xrange(input_dims):\n",
    "                pd_error_wrt_weight = deltas[i]*inputs[j]\n",
    "                w_grads[i,j] = pd_error_wrt_weight\n",
    "        return w_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "10 hidden dimensions, 1 output dimensions, 0.5 learning rate\n",
      "=== Hidden Layer ===\n",
      "weights, bias: [(array([ 0.22681506, -0.26632548]), 0.0), (array([-0.04476231,  0.20646326]), 0.0), (array([-0.42183681, -0.34732016]), 0.0), (array([-0.41244575,  0.05585466]), 0.0), (array([-0.18444234,  0.14850909]), 0.0), (array([-0.08968881,  0.11662964]), 0.0), (array([ 0.48337385, -0.09629922]), 0.0), (array([-0.23178698, -0.36546302]), 0.0), (array([-0.02381633, -0.40560307]), 0.0), (array([-0.20213714,  0.36287513]), 0.0)]\n",
      "gradients: [] + []\n",
      "outputs: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "None\n",
      "=== Output Layer ===\n",
      "weights, bias: [(array([ 0.34975978, -0.09268534, -0.04543018, -0.21615967,  0.40574569,\n",
      "       -0.24699519, -0.22203652,  0.18505457,  0.10227375,  0.18352193]), 0.0)]\n",
      "gradients: [] + []\n",
      "outputs: [ 0.]\n",
      "None\n",
      "Training...\n",
      "Done\n",
      "[ 0.97653043]\n",
      "[ 0.03444277]\n",
      "0.000323928385899\n",
      "10 hidden dimensions, 1 output dimensions, 0.5 learning rate\n",
      "=== Hidden Layer ===\n",
      "weights, bias: [(array([ 3.20405931, -5.50008761]), -1.2377801260513626), (array([-0.75734003, -0.95872275]), -0.24171943983515651), (array([-6.05959482, -6.05085037]), 2.1063638143128527), (array([-0.9175186 , -0.85992899]), -0.18691543161084664), (array([-4.56736407,  1.93578869]), -0.50894432892752506), (array([-0.64369576, -1.02272786]), -0.31058262427135613), (array([-0.01306883, -1.09880865]), -0.65600610071932453), (array([-0.46496813, -1.66676426]), 0.29913923721349556), (array([ 0.21612519, -2.50276965]), 0.15800330266881743), (array([-4.51280892,  1.99223028]), -0.58923508623331178)]\n",
      "gradients: [[  2.04675141e-04   2.04675141e-04]\n",
      " [  1.23338971e-04   1.23338971e-04]\n",
      " [ -4.43583627e-07  -4.43583627e-07]\n",
      " [  1.19319295e-04   1.19319295e-04]\n",
      " [  2.12212575e-04   2.12212575e-04]\n",
      " [  1.12034100e-04   1.12034100e-04]\n",
      " [  5.75827632e-05   5.75827632e-05]\n",
      " [  2.14562608e-04   2.14562608e-04]\n",
      " [  2.33344112e-04   2.33344112e-04]\n",
      " [  2.13734170e-04   2.13734170e-04]] + [  2.04675141e-04   1.23338971e-04  -4.43583627e-07   1.19319295e-04\n",
      "   2.12212575e-04   1.12034100e-04   5.75827632e-05   2.14562608e-04\n",
      "   2.33344112e-04   2.13734170e-04]\n",
      "outputs: [  2.83654356e-02   1.23707262e-01   4.52129688e-05   1.22995645e-01\n",
      "   4.14664575e-02   1.21638336e-01   1.45805724e-01   1.37929644e-01\n",
      "   1.06344060e-01   4.27042584e-02]\n",
      "None\n",
      "=== Output Layer ===\n",
      "weights, bias: [(array([ 6.46921512,  0.99122013, -8.54945003,  0.96368378,  4.65097104,\n",
      "        0.91353196,  0.40276364,  1.57194157,  2.13888269,  4.55441642]), -4.7591448145879873)]\n",
      "gradients: [[  3.25621625e-05   1.41990678e-04   5.18868095e-08   1.41173159e-04\n",
      "    4.76018330e-05   1.39613947e-04   1.67340417e-04   1.58333328e-04\n",
      "    1.22079660e-04   4.90228668e-05]] + [ 0.00114761]\n",
      "outputs: [ 0.03444277]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now let's test it! We'll start with something simple that even a single Perceptron could do.\n",
    "x_train = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y_train = [[0], [1], [1], [0]]\n",
    "nn = SimpleNN(input_dims = 2, hidden_dims = 10, output_dims = 1, learn_rate=0.5)\n",
    "print 'Before training'\n",
    "nn.dump_state()\n",
    "\n",
    "print 'Training...'\n",
    "mse = 100000\n",
    "num_itr = 0\n",
    "while mse > 0.0001 and num_itr < 5000:\n",
    "    history = nn.train(x_train, y_train)\n",
    "    mse = np.average([h[0] for h in history])\n",
    "    num_itr += 1\n",
    "print 'Done'\n",
    "    \n",
    "print nn.predict([0,1])\n",
    "print nn.predict([1,1])\n",
    "print mse\n",
    "nn.dump_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
